# -*- coding: utf-8 -*-
"""Predict Your Future Income Using Python & Machine Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fuur04-dG2fif6S_OC24A5WpjVpvMN6e
"""

#Description: This program attempts to predict a persons salary based on some attributes

# Import the libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
import seaborn as sns
import matplotlib.pyplot as plt

# Load the data
from google.colab import files
files.upload()

# Read the data
df = pd.read_csv('adult.csv') # adult.csv from Kaggle's Adult Census Income
# Show the data
df

# Get a coun of each marital status
df['marital.status'].value_counts()

# Visualize the count of each education type
sns.countplot(df['marital.status'], label = "count" )
plt.xticks(rotation = 90)

# Get the count of income above and below 50k for each marital status type
sns.countplot(x= 'marital.status', hue ='income', order = df['marital.status'].value_counts().index, data = df) 
plt.xticks(rotation = 90)
plt.show()

# Get the count of each income values within the data
df['income'].value_counts()

# Visually show the count of each income type above and below 50k
sns.countplot(x= 'income', hue ='income', order = df['income'].value_counts().index, data = df) 
plt.xticks(rotation = 90)
plt.show()

# Create a list of categorical columns to keep
categorical_keep_columns = ['age', 'workclass', 'education', 'relationship', 'marital.status']
#Show the data
df[categorical_keep_columns]

# Create a correlated number list of columns to keep form the categorical columns to train and test the model
keep_columns = ['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']
# Show the data
df[keep_columns]

# Split the data set into a feature or independent data set (X) and a target or dependent data set (Y)
X = df[keep_columns].values
Y = df['income'].values

# Split the data again but this time into 80% training and 20% testing data sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)

# Create and train the model
gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=1, max_features=2, max_depth=2, random_state=0)
# Train the model
gb_clf.fit(X_train, Y_train)

# Show the models score on the training and testing data sets
print('Accuracy score (training):', gb_clf.score(X_train, Y_train))
print('Accuracy score (test):', gb_clf.score(X_test, Y_test))

# Create a function to convert the data to number values (age, workclass, education, relationship, etc.)
def get_number(age, workclass, education, relationship):
  number_list = []

  number_list.append(df[df.age == age]['age'].values[0])
  # number_list.append(df[df.workclass == workclass]['workclass'].values[0])
  number_list.append(df[df.education == education]['education.num'].values[0])
  # number_list.append(df[df.relationship == relationship]['relationship'].values[0])
  # number_list.append(df[df.marital.status == marital.status]['marital.status'].values[0])

  return [number_list]

# Converting the categorical variable to numbers for the model
person = get_number(66, '?', 'Some-college', 'Unmarried')
# Show person
person

# Make the prediction and see what income type the model this person will have
gb_clf.predict(person)